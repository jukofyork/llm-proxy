# Minimal configuration demonstrating:
# - Bearer auth via api_key
# - Model allow-list
# - Defaults (applied if missing) and deny policy
# - Virtual profiles (suffix-based) with overrides
# - Hiding base models (only expose profile-suffixed models)
# - Default system/developer message upsert (added if missing)
# - Multi-node setups declared as separate servers (one endpoint each)
# - Local servers without auth

# OpenAI: hide base models; only expose profile-suffixed variants (e.g., gpt-4-fast, o1-preview-high)
[OpenAI]
endpoint = "https://api.openai.com/v1"
api_key = "sk-proj-..."
models = ["gpt-4", "o1-preview"]
defaults = { stream = true, stream_options = { include_usage = true } }
deny = ["/temperature"]
hide_base_models = true
default_system_message = "You are a helpful assistant. Keep answers concise."
default_developer_message = "Follow internal style guidelines."

[OpenAI.fast]
overrides = { stream = false, reasoning_effort = "low" }

[OpenAI.high]
overrides = { reasoning_effort = "high" }
default_system_message = "Think step-by-step and favor accuracy."

# Anthropic: standard base + two profiles; base models remain visible
[Anthropic]
endpoint = "https://api.anthropic.com/v1"
api_key = "sk-ant-..."
models = ["claude-3-5-sonnet-20241022"]
defaults = { stream = true }
default_system_message = "You are Claude. Be precise and safe."

[Anthropic.creative]
overrides = { temperature = 0.9, top_p = 0.95 }

[Anthropic.precise]
overrides = { temperature = 0.1, top_p = 0.8 }

# GPU Cluster: declare each node as its own server (single endpoint per server)
["GPU-Cluster-200"]
endpoint = "http://192.168.1.200:8080"
defaults = { stream = true }

["GPU-Cluster-201"]
endpoint = "http://192.168.1.201:8080"
defaults = { stream = true }

["GPU-Cluster-202"]
endpoint = "http://192.168.1.202:8080"
defaults = { stream = true }

# Local LLM: single node with deny policy and profiles
[LocalLLM-8080]
endpoint = "http://192.168.1.100:8080"
defaults = { stream = true, top_p = 0.9 }
deny = ["/top_k", "/repetition_penalty"]
default_system_message = "Local policy: avoid unsafe content."

[LocalLLM-8080.coding]
overrides = { temperature = 0.2 }

[LocalLLM-8080.creative]
overrides = { temperature = 0.8, top_p = 0.95 }

# Another Local LLM node (no profiles here to keep it minimal)
[LocalLLM-8081]
endpoint = "http://192.168.1.100:8081"
defaults = { stream = true, top_p = 0.9 }
deny = ["/top_k", "/repetition_penalty"]