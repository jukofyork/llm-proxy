# Example configuration showcasing virtual endpoints and parameter overrides

# ============================================================================
# CLOUD PROVIDERS WITH VIRTUAL ENDPOINTS
# ============================================================================

[OpenAI]
endpoint = "https://api.openai.com/v1"
api_key = "sk-proj-..."
models = ["gpt-4", "o1-preview"]
# Base parameters inherited by all virtual endpoints
params = { stream = true, stream_options = { include_usage = true } }
disallowed_params = ["temperature"]

# Virtual endpoints inherit base params and add/override specific ones
[OpenAI-high-reasoning]
params = { reasoning_effort = "high" }

[OpenAI-medium-reasoning]
params = { reasoning_effort = "medium" }

[OpenAI-fast]
params = { reasoning_effort = "low", stream = false }  # Overrides base stream=true

[Anthropic]
endpoint = "https://api.anthropic.com/v1"
api_key = "sk-ant-..."
models = ["claude-3-5-sonnet-20241022"]
# Base parameters for consistent behavior
params = { max_tokens = 4096, stream = true }

# Virtual endpoints for different creativity levels
[Anthropic-creative]
params = { temperature = 0.9, top_p = 0.95 }  # Inherits max_tokens and stream

[Anthropic-precise]
params = { temperature = 0.1, top_p = 0.8, max_tokens = 2048 }  # Overrides max_tokens

# ============================================================================
# LOCAL SERVERS WITH VIRTUAL CONFIGURATIONS
# ============================================================================

[LocalLLM]
endpoint = "http://192.168.1.100:8080"
# Base settings for all virtual endpoints
params = { stream = true, top_p = 0.9 }

# Virtual endpoints for different use cases
[LocalLLM-coding]
params = { temperature = 0.2, max_tokens = 8192 }  # Inherits stream and top_p
disallowed_params = ["top_k", "repetition_penalty"]

[LocalLLM-creative]
params = { temperature = 0.8, max_tokens = 4096, top_p = 0.95 }  # Overrides top_p

[LocalLLM-chat]
params = { temperature = 0.7, max_tokens = 2048 }  # Inherits stream and top_p

# ============================================================================
# MULTI-PORT SERVERS WITH VIRTUAL VARIANTS
# ============================================================================

[GPU-Cluster]
endpoint = "http://192.168.1.200"
ports = [8080, 8081, 8082]
# Base parameters inherited by virtual endpoints
params = { temperature = 0.7, stream = true }

# Virtual endpoints apply to all ports and inherit base params
[GPU-Cluster-fast]
params = { temperature = 0.3, max_tokens = 1024 }  # Inherits stream=true

[GPU-Cluster-quality]
params = { temperature = 0.8, max_tokens = 4096 }  # Inherits stream=true

# ============================================================================
# SPECIALIZED CONFIGURATIONS
# ============================================================================

[OpenRouter]
endpoint = "https://openrouter.ai/api/v1"
api_key = "sk-or-..."
models = ["anthropic/claude-3.5-sonnet", "meta-llama/llama-3.1-405b"]
params = { temperature = 0.7, stream = true }

[OpenRouter-cost-optimized]
params = { max_tokens = 1024, temperature = 0.5, stream = false }  # Overrides stream
disallowed_params = ["top_p", "top_k"]

[Local-Uncensored]
endpoint = "http://192.168.1.103:8080"
# No restrictions - useful for testing
params = { temperature = 0.8, stream = false }