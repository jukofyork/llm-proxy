# Minimal configuration demonstrating:
# - Bearer auth via api_key
# - Model allow-list
# - Defaults (applied if missing) and deny policy
# - Virtual profiles (suffix-based) with overrides
# - Multi-endpoint pool (implicit round-robin)
# - Local servers without auth and port expansion

[OpenAI]
endpoints = ["https://api.openai.com/v1"]
api_key = "sk-proj-..."
models = ["gpt-4", "o1-preview"]
defaults = { stream = true, stream_options = { include_usage = true } }
deny = ["/temperature"]

[OpenAI.fast]
overrides = { stream = false, reasoning_effort = "low" }

[OpenAI.high_reasoning]
overrides = { reasoning_effort = "high" }

[Anthropic]
endpoints = ["https://api.anthropic.com/v1"]
api_key = "sk-ant-..."
models = ["claude-3-5-sonnet-20241022"]
defaults = { stream = true }

[Anthropic.creative]
overrides = { temperature = 0.9, top_p = 0.95 }

[Anthropic.precise]
overrides = { temperature = 0.1, top_p = 0.8 }

["GPU-Cluster"]
endpoints = [
  "http://192.168.1.200:8080",
  "http://192.168.1.201:8080",
  "http://192.168.1.202:8080"
]
defaults = { stream = true }

["GPU-Cluster".quality]
overrides = { temperature = 0.8 }

[LocalLLM]
endpoint = "http://192.168.1.100"
ports = [8080, 8081]
defaults = { stream = true, top_p = 0.9 }
deny = ["/top_k", "/repetition_penalty"]

[LocalLLM.coding]
overrides = { temperature = 0.2 }

[LocalLLM.creative]
overrides = { temperature = 0.8, top_p = 0.95 }