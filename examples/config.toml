# Example configuration showcasing different server types and parameter features

# ============================================================================
# LOCAL SERVERS - Multi-port with shared configuration
# ============================================================================

[LOCAL-SERVER-1]
endpoint = "http://192.168.1.100"
ports = [8080, 8081]
# Block temperature parameter for all models on this server
disallowed_params = ["temperature"]
# Force streaming and set default temperature
params = { stream = true, temperature = 0.7 }

[LOCAL-SERVER-2]
endpoint = "http://192.168.1.101"
ports = [8080, 8081]
# Only allow specific parameters through
allowed_params = ["max_tokens", "top_p", "stream"]
# Override default parameters
params = { max_tokens = 2048, top_p = 0.9 }

# ============================================================================
# INDIVIDUAL PORT CONFIGURATIONS - Different settings per port
# ============================================================================

[LLAMA-SERVER-FAST]
endpoint = "http://192.168.1.102:8080"
# Fast inference settings
params = { temperature = 0.3, max_tokens = 1024, top_p = 0.8 }
disallowed_params = ["top_k", "repetition_penalty"]

[LLAMA-SERVER-QUALITY]
endpoint = "http://192.168.1.102:8081"
# Quality inference settings
params = { temperature = 0.8, max_tokens = 4096, top_p = 0.95 }
disallowed_params = ["top_k", "repetition_penalty"]

# ============================================================================
# CLOUD PROVIDERS - With API keys and model filtering
# ============================================================================

[OpenRouter]
endpoint = "https://openrouter.ai/api/v1"
api_key = "<YOUR KEY HERE>"
# Limit to specific high-quality models
models = ["anthropic/claude-3.5-sonnet", "anthropic/claude-3-opus", "google/gemini-pro", "meta-llama/llama-3.1-405b"]
# Remove cost-affecting parameters
disallowed_params = ["max_tokens"]
# Set reasonable defaults
params = { temperature = 0.7, top_p = 0.9 }

[OpenAI]
endpoint = "https://api.openai.com/v1"
api_key = "<YOUR KEY HERE>"
models = ["gpt-4", "gpt-4-turbo", "o1-preview"]
# o1 models don't support temperature
disallowed_params = ["temperature"]
# Enable usage tracking with nested parameters
params = { stream = true, reasoning_effort = "high", stream_options = { include_usage = true } }

[DeepSeek]
endpoint = "https://api.deepseek.com/v1"
api_key = "<YOUR KEY HERE>"
models = ["deepseek-chat", "deepseek-coder"]
# Force specific settings for consistency
params = { temperature = 0.7, max_tokens = 8192, stream = true }

# ============================================================================
# SPECIALIZED CONFIGURATIONS
# ============================================================================

[Anthropic-Creative]
endpoint = "https://api.anthropic.com/v1"
api_key = "<YOUR KEY HERE>"
models = ["claude-3-5-sonnet-20241022"]
# High creativity settings
params = { temperature = 0.9, max_tokens = 4096, top_p = 0.95 }
# Block parameters that reduce creativity
disallowed_params = ["top_k"]

[Anthropic-Precise]
endpoint = "https://api.anthropic.com/v1"
api_key = "<YOUR KEY HERE>"
models = ["claude-3-5-sonnet-20241022"]
# Low temperature for precise responses
params = { temperature = 0.1, max_tokens = 2048, top_p = 0.8 }

[Local-Uncensored]
endpoint = "http://192.168.1.103:8080"
# No parameter restrictions - pass everything through
# (no disallowed_params or allowed_params means no filtering)
params = { temperature = 0.8, repeat_penalty = 1.1, top_k = 40 }

# ============================================================================
# LOAD BALANCING EXAMPLE
# ============================================================================

[GPU-CLUSTER-NODE-1]
endpoint = "http://192.168.1.200:8080"
params = { temperature = 0.7, max_tokens = 4096 }

[GPU-CLUSTER-NODE-2]
endpoint = "http://192.168.1.201:8080"
params = { temperature = 0.7, max_tokens = 4096 }

[GPU-CLUSTER-NODE-3]
endpoint = "http://192.168.1.202:8080"
params = { temperature = 0.7, max_tokens = 4096 }

# ============================================================================
# DEVELOPMENT/TESTING
# ============================================================================

[Local-Debug]
endpoint = "http://localhost:8080"
# Allow all parameters for testing
params = { stream = false, temperature = 0.5 }
# No filtering - useful for development

[Mock-Server]
endpoint = "http://localhost:3000"
# Override all requests to use mock responses
params = { mock = true, response_delay = 100 }